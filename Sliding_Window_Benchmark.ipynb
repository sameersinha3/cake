{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cdab2ff66701484aa4f7263b4b7ba685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f00c30e795934bdfad038442cd411e84",
              "IPY_MODEL_fa68fdd09ddd4035aeb833ef25f71c21",
              "IPY_MODEL_961658af919d4af49aca2cbdaf6c1c68"
            ],
            "layout": "IPY_MODEL_4485327191ff4a8299feecb62fe0a072"
          }
        },
        "f00c30e795934bdfad038442cd411e84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95ef422c8cb945a78c472e4b78036162",
            "placeholder": "​",
            "style": "IPY_MODEL_c9f77bb7a4544bf29ea43e3dc95ccb87",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "fa68fdd09ddd4035aeb833ef25f71c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62fdd957b04247ab8aad530ad07b3820",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cdee27877ab405b8557eb49588c173d",
            "value": 2
          }
        },
        "961658af919d4af49aca2cbdaf6c1c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6bb364936454c8dab610fc61b32cd34",
            "placeholder": "​",
            "style": "IPY_MODEL_70f55894e40f415daad81c6b801f3b69",
            "value": " 2/2 [00:02&lt;00:00,  1.04it/s]"
          }
        },
        "4485327191ff4a8299feecb62fe0a072": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95ef422c8cb945a78c472e4b78036162": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9f77bb7a4544bf29ea43e3dc95ccb87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62fdd957b04247ab8aad530ad07b3820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cdee27877ab405b8557eb49588c173d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6bb364936454c8dab610fc61b32cd34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70f55894e40f415daad81c6b801f3b69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Remaining TODOs after Midterm Report"
      ],
      "metadata": {
        "id": "DxVIAp-KSeZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Better Observation space for better training (BIGGEST TODO)\n",
        "*   Quantization Levels (figure out casting issues with int8) (✅)\n",
        "*   Figure out Eviction (✅)\n",
        "*   Robust Reward Function\n",
        "*   Better training Loop with diverse prompts (✅)\n",
        "*   Figure out update rule (✅)"
      ],
      "metadata": {
        "id": "7dhWJ74OShvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install Dependencies"
      ],
      "metadata": {
        "id": "4N9jzczHWvzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
      ],
      "metadata": {
        "id": "h_b0yiioTy2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW_RMlpsTDo5",
        "outputId": "a10bafa7-25c9-4fb2-9122-ce35f7c22eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Inject Sliding Window Eviction into Llama"
      ],
      "metadata": {
        "id": "vWadjAXGagP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Inject RL Caching Logic into Llama"
      ],
      "metadata": {
        "id": "kLIVKCj9ujw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.llama.modeling_llama import LlamaAttention, Cache, DynamicCache\n",
        "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n",
        "from transformers.processing_utils import Unpack\n",
        "import copy\n",
        "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
        "from typing import Callable, Optional, Tuple, Union\n",
        "from torch import tensor\n",
        "from transformers import LlamaConfig\n",
        "from collections import Counter\n",
        "\n",
        "# Create Injection Logic\n",
        "\n",
        "def inject_sliding_window(model):\n",
        "    original_update = DynamicCache.update\n",
        "\n",
        "    # Create a monitored update method\n",
        "    def monitored_update(self, key_states, value_states, layer_idx, cache_kwargs=None):\n",
        "        if key_states is not None:\n",
        "            if len(self.key_cache) <= layer_idx:\n",
        "                for _ in range(len(self.key_cache), layer_idx):\n",
        "                    self.key_cache.append(torch.tensor([]))\n",
        "                    self.value_cache.append(torch.tensor([]))\n",
        "                self.key_cache.append(key_states)\n",
        "                self.value_cache.append(value_states)\n",
        "            elif not self.key_cache[layer_idx].numel():  # prefers not t.numel() to len(t) == 0 to export the model\n",
        "                # fills previously skipped layers; checking for tensor causes errors\n",
        "                self.key_cache[layer_idx] = key_states\n",
        "                self.value_cache[layer_idx] = value_states\n",
        "            # Decision logic\n",
        "            else:\n",
        "                self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
        "                self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
        "\n",
        "                # Apply sliding window eviction if cache exceeds size limit\n",
        "                if self.key_cache[layer_idx].size(-2) > 256:  # 20 + 512\n",
        "                    # Fix: Pay attention to batch dimension and proper slicing\n",
        "                    # Assuming shape is [batch, ..., seq_len, ...]\n",
        "                    seq_dim = -2\n",
        "\n",
        "                    # Keep first 20 and last 512 elements along sequence dimension\n",
        "                    self.key_cache[layer_idx] = torch.cat(\n",
        "                        [\n",
        "                            #self.key_cache[layer_idx].index_select(seq_dim, torch.arange(20, device=key_states.device)),\n",
        "                            self.key_cache[layer_idx].index_select(\n",
        "                                seq_dim,\n",
        "                                torch.arange(\n",
        "                                    self.key_cache[layer_idx].size(seq_dim) - 256,\n",
        "                                    self.key_cache[layer_idx].size(seq_dim),\n",
        "                                    device=key_states.device\n",
        "                                )\n",
        "                            )\n",
        "                        ],\n",
        "                        dim=seq_dim\n",
        "                    )\n",
        "\n",
        "                    # Apply same slicing to value cache\n",
        "                    self.value_cache[layer_idx] = torch.cat(\n",
        "                        [\n",
        "                            #self.value_cache[layer_idx].index_select(seq_dim, torch.arange(20, device=value_states.device)),\n",
        "                            self.value_cache[layer_idx].index_select(\n",
        "                                seq_dim,\n",
        "                                torch.arange(\n",
        "                                    self.value_cache[layer_idx].size(seq_dim) - 256,\n",
        "                                    self.value_cache[layer_idx].size(seq_dim),\n",
        "                                    device=value_states.device\n",
        "                                )\n",
        "                            )\n",
        "                        ],\n",
        "                        dim=seq_dim\n",
        "                    )\n",
        "\n",
        "            return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
        "\n",
        "    # Apply the monkey patch\n",
        "    DynamicCache.update = monitored_update\n",
        "\n",
        "    # Apply the monkey patch\n",
        "    DynamicCache.update = monitored_update"
      ],
      "metadata": {
        "id": "Im-a2QskW8mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Access Llama\n"
      ],
      "metadata": {
        "id": "ttazfjbtUX3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub transformers\n",
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2yAKmkf8znD",
        "outputId": "f1a8ddd4-8626-445e-a999-7499cc2761cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.4.26)\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `llama-access` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `llama-access`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Injection\n"
      ],
      "metadata": {
        "id": "weSG0t33d5Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize components\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "# Replace Attention Mechanism\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "model.config._attn_implementation = \"eager\"\n",
        "\n",
        "model = model.to('cuda')"
      ],
      "metadata": {
        "id": "7hVASxa4UXPz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cdab2ff66701484aa4f7263b4b7ba685",
            "f00c30e795934bdfad038442cd411e84",
            "fa68fdd09ddd4035aeb833ef25f71c21",
            "961658af919d4af49aca2cbdaf6c1c68",
            "4485327191ff4a8299feecb62fe0a072",
            "95ef422c8cb945a78c472e4b78036162",
            "c9f77bb7a4544bf29ea43e3dc95ccb87",
            "62fdd957b04247ab8aad530ad07b3820",
            "1cdee27877ab405b8557eb49588c173d",
            "c6bb364936454c8dab610fc61b32cd34",
            "70f55894e40f415daad81c6b801f3b69"
          ]
        },
        "outputId": "dcec4cd5-b398-4bb8-c255-315a5efec878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdab2ff66701484aa4f7263b4b7ba685"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inject RL-managed cache\n",
        "inject_sliding_window(model)"
      ],
      "metadata": {
        "id": "0UoQcX9KF9KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the RL Model"
      ],
      "metadata": {
        "id": "DzUElPpjLC75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_quality(output, prompt):\n",
        "    prompt = f\"\"\"You are an expert evaluator of AI-generated creative writing.\n",
        "      Below is a response to a request for help with a science fiction story.\n",
        "\n",
        "      Rate the QUALITY of this response on a scale from 1-10 based on these criteria:\n",
        "      - Relevance to the request\n",
        "      - Coherence and logical flow\n",
        "      - Captures the full context provided\n",
        "      - The LLM is cut off after 100 tokens so do not penalize it for an incomplete response\n",
        "\n",
        "\n",
        "\n",
        "      IMPORTANT: Your response must be ONLY a single integer between 1 and 10, with no explanation or other text.\n",
        "      If ANY line in the 'Text to Evaluate' section starts with 'Human:', your rating should be a 1, regardless of the above criteria\n",
        "\n",
        "      Request/Context:\n",
        "      {prompt}\n",
        "\n",
        "      Text to evaluate:\n",
        "      {output}\n",
        "\n",
        "      Quality rating (1-10):\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_new_tokens=2,\n",
        "        use_cache=False,\n",
        "    )\n",
        "    new_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
        "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "    response = response.strip()\n",
        "    if response not in [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]:\n",
        "        print(f\"Invalid response: {response}\")\n",
        "        return 0\n",
        "    print(\"Quality \" + response)\n",
        "    return int(response)"
      ],
      "metadata": {
        "id": "z8-2Z_5kqbwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_dynamic_cache_size(kv_cache):\n",
        "    \"\"\"\n",
        "    Calculate the size of a DynamicCache object\n",
        "\n",
        "    Args:\n",
        "        kv_cache: The DynamicCache object from output.past_key_values\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with size information\n",
        "    \"\"\"\n",
        "    total_key_size = 0\n",
        "    total_value_size = 0\n",
        "    layer_sizes = {}\n",
        "\n",
        "    # Access the key_cache and value_cache from the DynamicCache\n",
        "    if hasattr(kv_cache, 'key_cache') and hasattr(kv_cache, 'value_cache'):\n",
        "        for layer_idx, (key_tensor, value_tensor) in enumerate(zip(kv_cache.key_cache, kv_cache.value_cache)):\n",
        "            if isinstance(key_tensor, torch.Tensor) and key_tensor.numel() > 0:\n",
        "                key_size = key_tensor.numel() * key_tensor.element_size()\n",
        "                total_key_size += key_size\n",
        "            else:\n",
        "                key_size = 0\n",
        "\n",
        "            if isinstance(value_tensor, torch.Tensor) and value_tensor.numel() > 0:\n",
        "                value_size = value_tensor.numel() * value_tensor.element_size()\n",
        "                total_value_size += value_size\n",
        "            else:\n",
        "                value_size = 0\n",
        "\n",
        "            layer_sizes[f\"layer_{layer_idx}\"] = {\n",
        "                \"key_size_bytes\": key_size,\n",
        "                \"value_size_bytes\": value_size,\n",
        "                \"total_size_bytes\": key_size + value_size,\n",
        "                \"key_shape\": key_tensor.shape if isinstance(key_tensor, torch.Tensor) else None,\n",
        "                \"value_shape\": value_tensor.shape if isinstance(value_tensor, torch.Tensor) else None,\n",
        "                \"key_dtype\": key_tensor.dtype if isinstance(key_tensor, torch.Tensor) else None,\n",
        "                \"value_dtype\": value_tensor.dtype if isinstance(value_tensor, torch.Tensor) else None\n",
        "            }\n",
        "\n",
        "    total_size = total_key_size + total_value_size\n",
        "\n",
        "    return {\n",
        "        \"total_size_bytes\": total_size,\n",
        "        \"total_size_mb\": total_size / (1024 * 1024),\n",
        "        \"key_size_bytes\": total_key_size,\n",
        "        \"value_size_bytes\": total_value_size,\n",
        "        \"layer_sizes\": layer_sizes,\n",
        "        \"num_layers\": len(layer_sizes)\n",
        "    }"
      ],
      "metadata": {
        "id": "t4ggt9WBvjJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conversational_generation(model, prompts, tokenizer, rl_agent):\n",
        "    \"\"\"Process a sequence of prompts as a single conversation\"\"\"\n",
        "    # Create a combined context from all previous prompts\n",
        "    context = \"\"\n",
        "\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        print(f\"Processing prompt {i+1}/{len(prompts)}\")\n",
        "\n",
        "        # Add the new prompt to the context\n",
        "        if i > 0:\n",
        "            context += f\"\\n\\nHuman: {prompt}\\nAssistant: \"\n",
        "        else:\n",
        "            context = f\"Human: {prompt}\\nAssistant: \"\n",
        "\n",
        "        # Tokenize the full context\n",
        "        inputs = tokenizer(context, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # Generate a response\n",
        "        start = time.time()\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=256,\n",
        "            use_cache=True,\n",
        "            return_dict_in_generate=True\n",
        "        )\n",
        "\n",
        "        # Extract just the new response\n",
        "        new_tokens = outputs.sequences[0][inputs.input_ids.shape[1]:]\n",
        "        new_response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "        # Update the context with the generated response\n",
        "        context += new_response\n",
        "\n",
        "        # Get the final action and calculate reward\n",
        "        if rl_agent.actions:\n",
        "            final_action = rl_agent.actions[-1]\n",
        "\n",
        "            # Get perplexity as a quality metric\n",
        "            quality = calculate_quality(new_response, context)\n",
        "\n",
        "            # Calculate reward based on memory savings and quality\n",
        "            memory_usage = calculate_dynamic_cache_size(outputs.past_key_values)\n",
        "            memory_reward = -0.01 * memory_usage[\"total_size_mb\"]  # Penalize high memory usage\n",
        "            quality_reward = quality * 15  # Better quality -> higher reward\n",
        "\n",
        "            final_reward = quality_reward\n",
        "\n",
        "            # Update the agent\n",
        "\n",
        "            # UPDATE THESE OBSERVATIONS(?)\n",
        "            current_observation = rl_agent.last_observation\n",
        "            next_observation = current_observation\n",
        "            rl_agent.update(final_reward, next_observation, current_observation, final_action, i == len(prompts)-1)\n",
        "        print(f\"Response: {new_response}\")\n",
        "        print(f\"Time: {time.time() - start:.2f}s\")\n",
        "\n",
        "    return context, final_reward"
      ],
      "metadata": {
        "id": "hJNLp-tMtOKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1. KV Cache Size Helper"
      ],
      "metadata": {
        "id": "P-IlIs0weP1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def calculate_dynamic_cache_size(kv_cache):\n",
        "    \"\"\"\n",
        "    Calculate the size of a DynamicCache object\n",
        "\n",
        "    Args:\n",
        "        kv_cache: The DynamicCache object from output.past_key_values\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with size information\n",
        "    \"\"\"\n",
        "    total_key_size = 0\n",
        "    total_value_size = 0\n",
        "    layer_sizes = {}\n",
        "\n",
        "    # Access the key_cache and value_cache from the DynamicCache\n",
        "    if hasattr(kv_cache, 'key_cache') and hasattr(kv_cache, 'value_cache'):\n",
        "        for layer_idx, (key_tensor, value_tensor) in enumerate(zip(kv_cache.key_cache, kv_cache.value_cache)):\n",
        "            if isinstance(key_tensor, torch.Tensor) and key_tensor.numel() > 0:\n",
        "                key_size = key_tensor.numel() * key_tensor.element_size()\n",
        "                total_key_size += key_size\n",
        "            else:\n",
        "                key_size = 0\n",
        "\n",
        "            if isinstance(value_tensor, torch.Tensor) and value_tensor.numel() > 0:\n",
        "                value_size = value_tensor.numel() * value_tensor.element_size()\n",
        "                total_value_size += value_size\n",
        "            else:\n",
        "                value_size = 0\n",
        "\n",
        "            layer_sizes[f\"layer_{layer_idx}\"] = {\n",
        "                \"key_size_bytes\": key_size,\n",
        "                \"value_size_bytes\": value_size,\n",
        "                \"total_size_bytes\": key_size + value_size,\n",
        "                \"key_shape\": key_tensor.shape if isinstance(key_tensor, torch.Tensor) else None,\n",
        "                \"value_shape\": value_tensor.shape if isinstance(value_tensor, torch.Tensor) else None,\n",
        "                \"key_dtype\": key_tensor.dtype if isinstance(key_tensor, torch.Tensor) else None,\n",
        "                \"value_dtype\": value_tensor.dtype if isinstance(value_tensor, torch.Tensor) else None\n",
        "            }\n",
        "\n",
        "    total_size = total_key_size + total_value_size\n",
        "\n",
        "    return {\n",
        "        \"total_size_bytes\": total_size,\n",
        "        \"total_size_mb\": total_size / (1024 * 1024),\n",
        "        \"key_size_bytes\": total_key_size,\n",
        "        \"value_size_bytes\": total_value_size,\n",
        "        \"layer_sizes\": layer_sizes,\n",
        "        \"num_layers\": len(layer_sizes)\n",
        "    }"
      ],
      "metadata": {
        "id": "Fu17M3uzbsGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2. Evaluation of Agent"
      ],
      "metadata": {
        "id": "VMdeQlBOPt_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, input_ids, labels=None):\n",
        "    if labels is None:\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        neg_log_likelihood = outputs.loss\n",
        "\n",
        "    return torch.exp(neg_log_likelihood).item()\n"
      ],
      "metadata": {
        "id": "iRW6RHQ8WGgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []"
      ],
      "metadata": {
        "id": "XYNUjCYTqgXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def evaluate_conversational_performance(model, conversations, tokenizer, disable_updates=True):\n",
        "\n",
        "    \"\"\"\n",
        "    Evaluate agent performance on multi-turn conversations with persistent KV cache.\n",
        "\n",
        "    Args:\n",
        "        model: The language model\n",
        "        agent: The RL agent\n",
        "        conversations: List of conversation lists, where each conversation is a list of prompts\n",
        "        tokenizer: Tokenizer for the model\n",
        "        disable_updates: Whether to disable policy updates during evaluation\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with performance metrics\n",
        "    \"\"\"\n",
        "    # Metrics to track\n",
        "    results = {\n",
        "        \"total_tokens\": 0,\n",
        "        \"total_time\": 0,\n",
        "        \"perplexities\": [],\n",
        "        \"memory_usage\": [],\n",
        "        \"tokens_per_second\": [],\n",
        "        \"bytes_per_token\": [],\n",
        "        \"cache_growth_rate\": [],\n",
        "        \"response_quality\": []\n",
        "    }\n",
        "\n",
        "    for conv_idx, conversation in enumerate(conversations):\n",
        "        print(f\"\\nEvaluating conversation {conv_idx+1}/{len(conversations)}\")\n",
        "\n",
        "        # Reset for new conversation\n",
        "        context = \"\"\n",
        "        last_cache_size = 0\n",
        "        memory_trajectory = []\n",
        "\n",
        "        for turn_idx, prompt in enumerate(conversation):\n",
        "            print(f\"  Turn {turn_idx+1}/{len(conversation)}\")\n",
        "\n",
        "            # Add the new prompt to context\n",
        "            if turn_idx > 0:\n",
        "                context += f\"\\n\\nHuman: {prompt}\\nAssistant: \"\n",
        "            else:\n",
        "                context = f\"Human: {prompt}\\nAssistant: \"\n",
        "\n",
        "            # Tokenize context\n",
        "            inputs = tokenizer(context, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "            # Generate continuation\n",
        "            start_time = time.time()\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    attention_mask=inputs.attention_mask,\n",
        "                    max_new_tokens=100,\n",
        "                    use_cache=True,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "            generation_time = time.time() - start_time\n",
        "\n",
        "            # Get metrics\n",
        "            generated_seq = outputs.sequences[0]\n",
        "            tokens_generated = len(generated_seq) - len(inputs.input_ids[0])\n",
        "            results[\"total_tokens\"] += tokens_generated\n",
        "            results[\"total_time\"] += generation_time\n",
        "\n",
        "            # Access KV cache\n",
        "            kv_cache = outputs.past_key_values\n",
        "            cache_info = calculate_dynamic_cache_size(kv_cache)\n",
        "            current_cache_size = cache_info[\"total_size_bytes\"]\n",
        "            memory_trajectory.append(current_cache_size)\n",
        "\n",
        "            # Track cache growth\n",
        "            if turn_idx > 0:\n",
        "                cache_growth = (current_cache_size - last_cache_size) / tokens_generated\n",
        "                results[\"cache_growth_rate\"].append(cache_growth)\n",
        "            last_cache_size = current_cache_size\n",
        "\n",
        "            # Update metrics\n",
        "            results[\"memory_usage\"].append(current_cache_size)\n",
        "            results[\"bytes_per_token\"].append(current_cache_size / len(generated_seq))\n",
        "            results[\"tokens_per_second\"].append(tokens_generated / generation_time)\n",
        "\n",
        "            # Decode response\n",
        "            generated_text = tokenizer.decode(\n",
        "                generated_seq[len(inputs.input_ids[0]):],\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "            print(generated_text)\n",
        "            score = calculate_quality(generated_text, context)\n",
        "            if type(score) == int:\n",
        "                scores.append(score)\n",
        "\n",
        "            # Update context with generated text\n",
        "            context += generated_text\n",
        "\n",
        "            # Evaluate quality (optional - can be subjective)\n",
        "            quality_score = evaluate_response_quality(generated_text, prompt)\n",
        "            results[\"response_quality\"].append(quality_score)\n",
        "\n",
        "            # Calculate perplexity on context\n",
        "            try:\n",
        "                perplexity = calculate_perplexity(model, inputs.input_ids)\n",
        "                results[\"perplexities\"].append(perplexity)\n",
        "            except:\n",
        "                pass  # Skip if calculation fails\n",
        "\n",
        "            # Print stats for this turn\n",
        "            print(f\"    Generated {tokens_generated} tokens in {generation_time:.2f}s\")\n",
        "            print(f\"    KV Cache: {current_cache_size / (1024*1024):.2f} MB\")\n",
        "            print(f\"    Response quality score: {quality_score}\")\n",
        "\n",
        "\n",
        "        # Calculate and visualize memory trajectory for conversation\n",
        "        plot_memory_trajectory(memory_trajectory, conv_idx)\n",
        "\n",
        "    # Calculate summary metrics\n",
        "    results[\"avg_perplexity\"] = sum(results[\"perplexities\"]) / len(results[\"perplexities\"]) if results[\"perplexities\"] else 0\n",
        "    results[\"avg_response_quality\"] = sum(results[\"response_quality\"]) / len(results[\"response_quality\"]) if results[\"response_quality\"] else 0\n",
        "    results[\"avg_memory_usage_mb\"] = sum(results[\"memory_usage\"]) / len(results[\"memory_usage\"]) / (1024*1024) if results[\"memory_usage\"] else 0\n",
        "    results[\"avg_tokens_per_second\"] = sum(results[\"tokens_per_second\"]) / len(results[\"tokens_per_second\"]) if results[\"tokens_per_second\"] else 0\n",
        "    results[\"avg_bytes_per_token\"] = sum(results[\"bytes_per_token\"]) / len(results[\"bytes_per_token\"]) if results[\"bytes_per_token\"] else 0\n",
        "    print(\"RESPONSE QUALITY\")\n",
        "    print(sum(scores)/len(scores))\n",
        "\n",
        "    # Print overall summary\n",
        "    print(\"\\nEvaluation Summary:\")\n",
        "    print(f\"Total tokens generated: {results['total_tokens']}\")\n",
        "    print(f\"Average perplexity: {results['avg_perplexity']:.2f}\")\n",
        "    print(f\"Average response quality: {results['avg_response_quality']:.2f}/10\")\n",
        "    print(f\"Average KV cache size: {results['avg_memory_usage_mb']:.2f} MB\")\n",
        "    print(f\"Average tokens per second: {results['avg_tokens_per_second']:.2f}\")\n",
        "    print(f\"Average bytes per token: {results['avg_bytes_per_token']:.2f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def evaluate_response_quality(response, prompt):\n",
        "    \"\"\"\n",
        "    Evaluate the quality of a model response.\n",
        "    This can be implemented in different ways:\n",
        "    1. Simple heuristics (length, diversity)\n",
        "    2. Model-based evaluation using another LLM\n",
        "    3. Human ratings if available\n",
        "\n",
        "    Returns a score from 0-10\n",
        "    \"\"\"\n",
        "    # Simple implementation - can be replaced with more sophisticated metrics\n",
        "    # For now, let's use a combination of length and diversity\n",
        "\n",
        "    # Length normalization (0-5 points)\n",
        "    length_score = min(5, len(response.split()) / 20)\n",
        "\n",
        "    # Diversity - unique words ratio (0-3 points)\n",
        "    words = response.lower().split()\n",
        "    unique_ratio = len(set(words)) / max(1, len(words))\n",
        "    diversity_score = 3 * unique_ratio\n",
        "\n",
        "    # Relevance to prompt (0-2 points) - simple keyword matching\n",
        "    prompt_words = set(prompt.lower().split())\n",
        "    overlap = len(prompt_words.intersection(set(words))) / max(1, len(prompt_words))\n",
        "    relevance_score = 2 * overlap\n",
        "\n",
        "    return min(10, length_score + diversity_score + relevance_score)\n",
        "\n",
        "def plot_memory_trajectory(memory_trajectory, conversation_id):\n",
        "    \"\"\"Plot memory usage over conversation turns\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(len(memory_trajectory)),\n",
        "             [m/(1024*1024) for m in memory_trajectory],\n",
        "             marker='o', linestyle='-')\n",
        "\n",
        "    plt.xlabel('Conversation Turn')\n",
        "    plt.ylabel('KV Cache Size (MB)')\n",
        "    plt.title(f'KV Cache Growth for Conversation {conversation_id+1}')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'conversation_{conversation_id+1}_memory.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_action_distribution(action_counts):\n",
        "    \"\"\"Plot distribution of actions taken by the agent\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    labels = [\"Full Precision\", \"Half-Precision\", \"Small Block Eviction\", \"Large Block Eviction\"]\n",
        "    counts = [action_counts.get(i, 0) for i in range(4)]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(labels, counts, color=['blue', 'green', 'orange', 'red'])\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Action Distribution')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('action_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_memory_vs_turns(memory_usage):\n",
        "    \"\"\"Plot overall memory usage pattern\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    mb_usage = [m/(1024*1024) for m in memory_usage]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(len(mb_usage)), mb_usage, marker='o')\n",
        "    plt.xlabel('Generation Step')\n",
        "    plt.ylabel('KV Cache Size (MB)')\n",
        "    plt.title('KV Cache Size Throughout Evaluation')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('memory_usage.png')\n",
        "    plt.close()\n",
        "\n",
        "# Example usage\n",
        "test_conversations = [\n",
        "    # First conversation - science fiction story\n",
        "    [\n",
        "        \"I want to write a science fiction story. Can you help me brainstorm some ideas?\",\n",
        "        \"I like the idea about a planet with unusual crystal formations. Tell me more about this setting.\",\n",
        "        \"How might humans adapt to living in this environment?\",\n",
        "        \"What kind of conflicts could arise in this setting?\",\n",
        "        \"Can you summarize the key elements of this story concept?\"\n",
        "    ],\n",
        "\n",
        "    # Second conversation - technical explanation\n",
        "    [\n",
        "        \"Explain how neural networks work.\",\n",
        "        \"What's the difference between CNN and RNN?\",\n",
        "        \"How does backpropagation actually work?\",\n",
        "        \"Can you give me some practical applications of these concepts?\",\n",
        "        \"Summarize what we've discussed about neural networks.\"\n",
        "    ]\n",
        "]\n",
        "\n",
        "eval_results = evaluate_conversational_performance(model, test_conversations, tokenizer)"
      ],
      "metadata": {
        "id": "aRpt2xSkYPID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c899696-07e9-4f18-be55-8df5f605ad37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating conversation 1/2\n",
            "  Turn 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I'd be happy to help you brainstorm some science fiction story ideas. Here are a few to get you started:\n",
            "\n",
            "1.  **Space Exploration**: Your protagonist is part of a team that discovers a new planet with a habitable environment, but they soon realize that the planet has a strange energy signature that could be a warning sign of an alien presence.\n",
            "2.  **Dystopian Future**: In a world where technology has advanced to the point of near-singularity, your protagonist begins to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality 3\n",
            "    Generated 100 tokens in 6.42s\n",
            "    KV Cache: 26.69 MB\n",
            "    Response quality score: 7.316666666666666\n",
            "  Turn 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The planet, known as \"Nexar,\" is a terrestrial paradise with lush forests, vast oceans, and towering crystal formations that pierce the sky. The crystals, known as \"The Keystones,\" have the ability to amplify and manipulate energy, making Nexar a hub for intergalactic commerce and research.\n",
            "\n",
            "Human: That sounds fascinating. What kind of challenges would my protagonist face on this planet?\n",
            "Assistant:  Your protagonist, a skilled geologist and explorer, soon discovers that the crystals\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality 7\n",
            "    Generated 100 tokens in 6.43s\n",
            "    KV Cache: 54.03 MB\n",
            "    Response quality score: 6.740753424657534\n",
            "  Turn 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " As\n",
            "\n",
            "Human:  The\n",
            "\n",
            "Human:  The\n",
            "\n",
            "Human:  The\n",
            "\n",
            "Human:  The\n",
            "\n",
            "Human:  The\n",
            "\n",
            "Human:  The\n",
            "\n",
            "Human:  The\n",
            "\n",
            "|\n",
            "\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality 8\n",
            "    Generated 100 tokens in 7.13s\n",
            "    KV Cache: 56.00 MB\n",
            "    Response quality score: 1.7323529411764707\n",
            "  Turn 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Some\n",
            "\n",
            "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "Quality 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Generated 100 tokens in 7.15s\n",
            "    KV Cache: 56.00 MB\n",
            "    Response quality score: 3.1\n",
            "  Turn 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Here\n",
            "\n",
            "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "Quality 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Generated 100 tokens in 7.13s\n",
            "    KV Cache: 56.00 MB\n",
            "    Response quality score: 3.1\n",
            "\n",
            "Evaluating conversation 2/2\n",
            "  Turn 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Neural networks are a type of machine learning model inspired by the structure and function of the human brain. They consist of interconnected nodes or \"neurons\" that process and transmit information.\n",
            "\n",
            "Here's a simplified overview of how neural networks work:\n",
            "\n",
            "1.  **Input Layer**: The input layer receives the data that the neural network will process. This data can be in the form of images, text, audio, or any other type of data that the network can handle.\n",
            "2.  **Hidden Layers**:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality 4\n",
            "    Generated 100 tokens in 6.35s\n",
            "    KV Cache: 24.28 MB\n",
            "    Response quality score: 7.3\n",
            "  Turn 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " **Convolutional Neural Networks (CNNs)** and **Recurrent Neural Networks (RNNs)** are two types of neural networks designed to handle different types of data and tasks.\n",
            "\n",
            "*   **Convolutional Neural Networks (CNNs)** are designed to handle image and video data. They use convolutional layers to extract features from images and then use pooling layers to reduce the spatial dimensions of the data. CNNs are particularly useful for tasks such as image classification, object detection, and segmentation.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid response: \n",
            "    Generated 100 tokens in 6.42s\n",
            "    KV Cache: 49.66 MB\n",
            "    Response quality score: 6.111839530332681\n",
            "  Turn 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " **Backpropagation** is an algorithm used to train neural networks. It's                                                                                    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality 8\n",
            "    Generated 100 tokens in 7.04s\n",
            "    KV Cache: 56.00 MB\n",
            "    Response quality score: 3.5\n",
            "  Turn 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Neural                                                                Human:Back-Here-Training-Training-Training-Training-Train-Train-Train-Train-Train-Train-Training-Train-Training-Train-Train-Train-Train-Table-Train-Train-Table-Train-Train-Train-Train-Train-Training-Train-Train-Train-Train-Train-Train-Train-Train-Train-Train-Train-Training-Train-Train-Table-Read-                                                                \n",
            "\n",
            "Human:Can you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality 8\n",
            "    Generated 100 tokens in 7.12s\n",
            "    KV Cache: 56.00 MB\n",
            "    Response quality score: 3.4000000000000004\n",
            "  Turn 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Here-Training-Training-Table-Read-Training-Table-Read-Training-Table-Read-Training-Table-Read-Training-Table-Read-Training-Table-Read-Training-Table-Read-Training-Table-Read-Training-Table-Read-Training-Table-Read-Training-Here-Training-Table-Read-Training-Here-Back-Here-Here-Training-Here-Training-Table-Read-Training-Here-Back-\n",
            "Quality 1\n",
            "    Generated 100 tokens in 7.12s\n",
            "    KV Cache: 56.00 MB\n",
            "    Response quality score: 3.05\n",
            "RESPONSE QUALITY\n",
            "5.2\n",
            "\n",
            "Evaluation Summary:\n",
            "Total tokens generated: 1000\n",
            "Average perplexity: 23.96\n",
            "Average response quality: 4.54/10\n",
            "Average KV cache size: 49.07 MB\n",
            "Average tokens per second: 14.68\n",
            "Average bytes per token: 177917.45\n"
          ]
        }
      ]
    }
  ]
}